{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для подсчета слов использовалась команда ```wc -w```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH -- путь к папке с исходными файлами\n",
    "\n",
    "!find PATH -name '*.txt' -type f | xargs wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее производилась обработка корпуса с помощью Ru Syntax (http://web-corpora.net/wsgi3/ru-syntax/), в результате были получены файл c разбором в формате ```.conll``` для каждого документа корпуса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из каждого полученного файла ```.conll``` были взяты лексемы и через ```_``` соединены с обозначением части речи (POS tagging). На выходе получились файлы с текстами каждого документа, в которых все слова стоят в начальной форме и рядом через ```_``` указана их часть речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH -- путь к папке с файлами .conll\n",
    "# PATH2 -- путь для сохранения полученных файлов\n",
    "\n",
    "for file in (os.listdir(PATH)):\n",
    "    data = pd.read_csv(PATH + file, sep='\\t',\n",
    "                       names=['id', 'form', 'lemma', 'upos',\n",
    "                              'xpos', 'feats', 'head', 'deprel',\n",
    "                              'deps', 'misc'], low_memory=False)\n",
    "    \n",
    "    data['lemma_pos'] = data['lemma'] + '_' + data['upos']\n",
    "    \n",
    "    with open(PATH2 + file.replace('.conll', '_lemma_pos.txt'), \n",
    "              'w', encoding='utf-8') as fw:\n",
    "        fw.write(' '.join(data['lemma_pos'].str.lower().astype('str')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого в файлах все числа, знаки препинания, вхождения, которые определились программой RU Syntax как nonlex или как однобуквенные существительные, и сокращение _гг_, _гг._ от года были заменены на тег ```<w>```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH -- путь к папке с файлами _lemma_pos.txt, полученным на предыдущем шаге\n",
    "# PATH2 -- путь для сохранения файлов\n",
    "\n",
    "pattern = re.compile('[^ ]*?(?:[«»()–,:]|_\\W|___|sent|num|nonlex)[^ ]*?')\n",
    "pattern2 = re.compile('\\s(([a-zа-я]|гг)\\.?_s)\\s')\n",
    "pattern3 = re.compile('<w> *?<w>')\n",
    "pattern4 = re.compile('(<w> ?){2,}')\n",
    "\n",
    "for file in os.listdir(PATH):\n",
    "    with open(PATH + file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        \n",
    "        cleaned = re.sub(pattern, '<w>', text)\n",
    "        cleaned2 = re.sub(pattern2, ' <w> ', cleaned)\n",
    "        cleaned3 = re.sub(pattern3, '<w>', cleaned2)\n",
    "        cleaned4 = re.sub(pattern4, '<w> ', cleaned3)\n",
    "    \n",
    "    with open(PATH2 + file, 'w', encoding='utf-8') as fw:\n",
    "        fw.write(cleaned4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее с помощью программы AntConc был составлен частотный список всех биграмм из текстов коллекции\n",
    "и отсортирован по убыванию частотности. \n",
    "\n",
    "Из этого списка были удалены все вхождения, частотность которых == 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH -- путь к списку биграмм\n",
    "\n",
    "data = pd.read_csv(PATH, names=['rank', 'freq', 'range', 'ngrams'],\n",
    "                   sep='\\t', low_memory=False).drop([0, 1], axis=0)\n",
    "data = data.drop(data[data['freq'] == 1].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также были удалены все вхождения, которые содержат тег ```<w>```, точку (инициалы или сокращения), показатель части речи без какого-либо слова или являются местоимением (кроме начинающихся на _с_, как возвратное _себя_, которое встречается в таких конструкциях, как _представлять себя_):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = data['ngrams'].str.contains('[^с]_spro|apro|\\W_[a-z]|<w>|\\.', regex = True)\n",
    "df = data.drop(pos[pos == True].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем были удалены все вхождения, которые не содержат показателя части речи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_pos = df['ngrams'].str.contains('_', regex = True)\n",
    "df = df.drop(not_pos[not_pos == False].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И наконец были удалены все биграммы, включающие слова из списка стоп-слов, который содержит преимущественно самые частотные союзы, местоимения, наречия и частицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH2 -- путь для сохранения файла\n",
    "\n",
    "stopwords = open('stopwords.txt').readlines()\n",
    "    \n",
    "for word in stopwords:\n",
    "    word = word.strip()\n",
    "    \n",
    "    with_stopwords1 = df['ngrams'].str.contains('^' + word, regex = True)\n",
    "    df = df.drop(with_stopwords1[with_stopwords1 == True].index)\n",
    "    \n",
    "    with_stopwords2 = df['ngrams'].str.contains(' ' + word, regex = True)\n",
    "    df = df.drop(with_stopwords2[with_stopwords2 == True].index)\n",
    "\n",
    "df = df.sort_values(['range', 'freq'], ascending=False)\n",
    "df.to_csv(PATH2, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве лингвистического фильтра были выбраны 7 морфологических шаблонов: Verb + Noun, Noun + Verb, Prep + Noun, Noun + Prep, Verb + Prep, Adj + Noun, Adv + Verb. \n",
    "Для каждого из шаблонов были извлечены все соответствующие им выражения из списка биграмм, с условием, что они встречаются не менее, чем в 5 документах коллекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае выделения выражений по морфологическим шаблонам, в которых присутствует глагол (Verb + Noun, Verb + Prep, Adv + Verb), в них включались конструкции и с тегом ```_partcp``` (для других морфологических шаблонов код отличался только регулярным выражением):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_bigrams -- путь к списку биграмм\n",
    "# PATH2 -- путь для сохранения файла\n",
    "\n",
    "data = pd.read_csv(PATH_bigrams, names=['id', 'freq', 'range', 'ngrams'],\n",
    "                   sep='\\t', low_memory=False)\n",
    "\n",
    "data = data[data['range'] > 4]\n",
    "\n",
    "prs = data['ngrams'].str.contains('\\w+?_(?:v|partcp)\\s\\w+?_pr', regex = True)\n",
    "df = data.drop(prs[prs == False].index)\n",
    "\n",
    "df.to_csv(PATH2, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате было получено 7 файлов с биграммами, соответствующими морфологическим шаблонам, с частотностью для каждой биграммы и количеством документов, в которых она встречаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для референтного корпуса были проделаны все те же шаги, что и для основного корпуса."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
