{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вычисление показателей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH_bigrams -- путь к папке с лемматизированными текстами основного корпуса\n",
    "# PATH_bigrams_lists -- путь к папке со списками биграмм\n",
    "# PATH_reference -- путь к папке с лемматизированными текстами референтного корпуса\n",
    "# PATH_reference_lists -- путь к папке со списками биграмм референтного корпуса\n",
    "# PATH2 -- путь для сохранения файлов\n",
    "# файлы, заканчивающиеся на _words.txt -- списки биграмм, очищенные от данных о частотности и тд"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для вычисления показателя Weirdness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weirdness(pos, PATH_bigrams_lists, PATH_reference_lists, PATH2):\n",
    "    corp = pd.read_csv('{}/bigrams_{}.txt'.format(PATH_bigrams_lists, pos), sep='\\t',\n",
    "                       names=['id', 'freq', 'range', 'ngram'], index_col=False)\n",
    "    ref = pd.read_csv('{}/reference_{}.txt'.format(PATH_reference_lists, pos), sep='\\t',\n",
    "                        names=['id', 'freq', 'range', 'ngram'], index_col=False).drop([0, 1], axis=0)\n",
    "    \n",
    "    df = pd.merge(corp, ref, on='ngram', suffixes=('_corp', '_ref')).drop(['id_corp', 'id_ref'], axis=1)\n",
    "    \n",
    "    df['size_corp'] = 1970426\n",
    "    df['size_ref'] = 1165252\n",
    "    \n",
    "    df['weird'] = (df['freq_corp']/df['size_corp'])/(df['freq_ref']/df['size_ref'])\n",
    "    \n",
    "    weirdness_sorted = df.sort_values('weirdness', ascending=False)\n",
    "    weirdness_sorted[['ngram',\n",
    "                      'weirdness',\n",
    "                      'freq_corp',\n",
    "                      'size_corp',\n",
    "                      'freq_ref',\n",
    "                      'size_ref']].to_csv(PATH2 + pos, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['s_v', 'v_s', 'pr_s', 's_pr', 'a_s', 'adv_v', 'v_pr']\n",
    "for pos in tqdm_notebook(pos_list):\n",
    "    count_weirdness(pos, PATH_bigrams_lists, PATH_reference_lists, PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для вычисления показателя G2 от LogLikelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_loglikelihood(pos, PATH_bigrams_lists, PATH_reference_lists, PATH2):\n",
    "    corp = pd.read_csv('{}/bigrams_{}.txt'.format(PATH_bigrams_lists, pos), sep='\\t',\n",
    "                       names=['id', 'freq', 'range', 'ngram'], index_col=False)\n",
    "    ref = pd.read_csv('{}/reference_{}.txt'.format(PATH_reference_lists, pos), sep='\\t',\n",
    "                        names=['id', 'freq', 'range', 'ngram'], index_col=False).drop([0, 1], axis=0)\n",
    "    \n",
    "    df = pd.merge(corp, ref, on='ngram', suffixes=('_corp', '_ref')).drop(['id_corp', 'id_ref'], axis=1)\n",
    "    \n",
    "    df['size_corp'] = 1970426\n",
    "    df['size_ref'] = 1165252\n",
    "    \n",
    "    df['e1'] = df['size_corp'] * (df['freq_corp'] + df['freq_ref']) / (df['size_corp'] + df['size_ref'])\n",
    "    df['e2'] = df['size_ref'] * (df['freq_corp'] + df['freq_ref']) / (df['size_corp'] + df['size_ref'])\n",
    "    df['g2'] = 2 * ((df['freq_corp'] * np.log(df['freq_corp'] / df['e1']))\n",
    "                  + (df['freq_ref'] * np.log(df['freq_ref'] / df['e2'])))\n",
    "    \n",
    "    g2_sorted = df.sort_values('g2', ascending=False)\n",
    "    g2_sorted[['ngram',\n",
    "               'g2',\n",
    "               'freq_corp',\n",
    "               'size_corp',\n",
    "               'freq_ref',\n",
    "               'size_ref']].to_csv(PATH2 + pos, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['s_v', 'v_s', 'pr_s', 's_pr', 'a_s', 'adv_v', 'v_pr']\n",
    "for pos in tqdm_notebook(pos_list):\n",
    "    count_loglikelihood(pos, PATH_bigrams_lists, PATH_reference_lists, PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для вычисления TF-IDF на основе внешнего корпуса:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление для каждой биграммы количества документов внешнего корпуса, в которых она встречается:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_of_documents(words, num_documents, PATH_reference):\n",
    "    for file in os.listdir(PATH_reference):\n",
    "        \n",
    "        with open(PATH_reference + file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "            for elem in words:\n",
    "                res = re.search(elem, text)\n",
    "                if res:\n",
    "                    num_documents[elem] += 1\n",
    "                else:\n",
    "                    num_documents[elem] == 1\n",
    "    return num_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['pr_s', 's_v', 'v_s', 's_pr', 'a_s', 'adv_v', 'v_pr']\n",
    "num_documents = defaultdict(lambda: 1)\n",
    "\n",
    "for pos in tqdm_notebook(pos_list):\n",
    "    with open('{}/bigrams_{}_words.txt'.format(PATH_bigrams, pos)) as f:\n",
    "        lines = f.read()\n",
    "        words = lines.split('\\n')\n",
    "        \n",
    "        num_documents = count_number_of_documents(words, num_documents, PATH_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчёт TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tf_idf(pos, num_documents, PATH_bigrams_lists, PATH2):\n",
    "    df = pd.read_csv('{}/bigrams_{}_words.txt'.format(PATH_bigrams_lists, pos), sep='\\t',\n",
    "                   names=['id', 'freq', 'range', 'ngram'], index_col=False)\n",
    "    \n",
    "    df['tf_idf'] = ''\n",
    "    for i in df.index:    \n",
    "        df['tf_idf'].loc[i] = df['freq'].loc[i] * np.log10(20 / num_documents[df['ngram'].loc[i]])\n",
    "    \n",
    "    tf_idf_sorted = df.sort_values('tf_idf', ascending=False)\n",
    "    tf_idf_sorted.to_csv(PATH2 + pos, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['s_v', 'v_s', 'pr_s', 's_pr', 'a_s', 'adv_v', 'v_pr']\n",
    "for pos in tqdm_notebook(pos_list):\n",
    "    count_tf_idf(pos, num_documents, PATH_bigrams_lists, PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для вычисления T-Score:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала с помощью программы AntConc был получен частотный список слов на основе исследуемого корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = pd.read_csv(PATH_bigrams_lists + 'wordlist.txt', sep='\\t',\n",
    "                   names=['id', 'freq_wordlist', 'word'], index_col=False).drop([0, 1, 2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_word_freqs(df, wordlist):  \n",
    "    \n",
    "    for i in tqdm_notebook(df.index):\n",
    "        df['first_word'].loc[i] = df['split'].loc[i][0]\n",
    "        df['sec_word'].loc[i] = df['split'].loc[i][1]\n",
    "    \n",
    "    df_w = pd.merge(df, wordlist[['freq_wordlist', 'word']],\n",
    "         left_on='first_word', right_on='word').drop('word', axis=1)\n",
    "    df_w = df_w.rename(columns={'freq_wordlist': 'freq_first_word'})\n",
    "    \n",
    "    df_w = pd.merge(df_w, wordlist[['freq_wordlist', 'word']],\n",
    "         left_on='sec_word', right_on='word').drop('word', axis=1)\n",
    "    df_w = df_w.rename(columns={'freq_wordlist': 'freq_sec_word'})\n",
    "    \n",
    "    return df_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_t_score(pos, PATH_bigrams, PATH2, wordlist):\n",
    "    df = pd.read_csv('{}/bigrams_{}_words.txt'.format(PATH_bigrams, pos), sep='\\t',\n",
    "                   names=['id', 'freq', 'range', 'ngram'], index_col=False)\n",
    "    \n",
    "    df['first_word'] = ''\n",
    "    df['sec_word'] = ''\n",
    "    df['split'] = df['ngram'].str.split()\n",
    "\n",
    "    df = add_word_freqs(df, wordlist)\n",
    "    \n",
    "    df_w['t-score'] = (df_w['freq'] / 125277 - (df_w['freq_first_word'] / 125277) * (df_w['freq_sec_word']\n",
    "                        / 125277)) / np.sqrt(df_w['freq'] / 125277 / 125277)\n",
    "    \n",
    "    t_score_sorted = df_w.sort_values('t-score', ascending=False)\n",
    "    t_score_sorted.to_csv(PATH2 + pos, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = ['s_v', 'v_s', 'pr_s', 's_pr', 'a_s', 'adv_v', 'v_pr']\n",
    "for pos in tqdm_notebook(pos_list):\n",
    "    count_t_score(pos, PATH_bigrams, PATH2, wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для вычисления C-Value и NC-Value:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала с помощью программы AntConc были получены списки 3-grams, 4-grams, 5-grams и 6-grams на основе исследуемого корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams3 = pd.read_csv('ngrams3.txt', names=['rank', 'freq', 'range', 'ngrams'],\n",
    "                      sep='\\t', low_memory=False).drop([0, 1], axis=0)\n",
    "ngrams4 = pd.read_csv('ngrams4.txt', names=['rank', 'freq', 'range', 'ngrams'],\n",
    "                      sep='\\t', low_memory=False).drop([0, 1], axis=0)\n",
    "ngrams5 = pd.read_csv('ngrams5.txt', names=['rank', 'freq', 'range', 'ngrams'],\n",
    "                      sep='\\t', low_memory=False).drop([0, 1], axis=0)\n",
    "ngrams6 = pd.read_csv('ngrams6.txt', names=['rank', 'freq', 'range', 'ngrams'],\n",
    "                      sep='\\t', low_memory=False).drop([0, 1], axis=0)\n",
    "\n",
    "data = pd.concat([ngrams6, ngrams5, ngrams4, ngrams3], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем создаем словарь, в котором для каждого сочетания двух слов из списков 3-grams, 4-grams, 5-grams и 6-grams записаны все ngrams, в состав которых оно входит, и частотность этих ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(lambda: {'freq': 0, 'ngrams': {}})\n",
    "\n",
    "for i in tqdm_notebook(data.index):\n",
    "    lst = data['ngrams'].loc[i].split()\n",
    "    \n",
    "    for idx in range (len(lst) - 1):\n",
    "        d[lst[idx] + ' ' + lst[idx+1]]['freq'] += data['freq'].loc[i]\n",
    "        d[lst[idx] + ' ' + lst[idx+1]]['ngrams'][data['ngrams'].loc[i]] = data['freq'].loc[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для подсчёта C-Value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_c_value_for_ngrams(d, df, d_for_ngrams):\n",
    "    for i in tqdm_notebook(df.index):\n",
    "        \n",
    "        if not d.get(df['ngram'].loc[i]):\n",
    "            df['c-value'].loc[i] = np.log2(2) * df['freq'].loc[i]\n",
    "        else:\n",
    "            df['c-value'].loc[i] = np.log2(2) * (df['freq'].loc[i] - d[df['ngram'].loc[i]]['freq'] /\n",
    "                                                 len(d[df['ngram'].loc[i]]['ngrams']))\n",
    "            d_for_ngrams = count_c_value_for_bigger_ngrams(d_for_ngrams, d, df, i)\n",
    "    \n",
    "    return df, d_for_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для подсчёта C-Value для 3-grams, 4-grams, 5-grams и 6-grams, если в них встречается биграмма, для которой уже вычислено C-Value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_c_value_for_bigger_ngrams(d_for_ngrams, d, df, i):\n",
    "    for elem in d[df['ngram'].loc[i]]['ngrams']:\n",
    "        bigger_ngrams = {'freq': 0, 'ngrams': {}}\n",
    "        \n",
    "        for b_elem in d[df['ngram'].loc[i]]['ngrams']:\n",
    "            if elem in b_elem and elem != b_elem:\n",
    "                bigger_ngrams['freq'] += d[df['ngram'].loc[i]]['ngrams'][b_elem]\n",
    "                bigger_ngrams['ngrams'][b_elem] = d[df['ngram'].loc[i]]['ngrams'][b_elem]\n",
    "        \n",
    "        d_for_ngrams['ngram'].append(elem)\n",
    "        \n",
    "        if bigger_ngrams['freq'] != 0:\n",
    "            d_for_ngrams['c-value'].append(np.log2(len(elem.split()) * (d[df['ngram'].loc[i]]['ngrams'][elem] -\n",
    "                                             bigger_ngrams['freq']/len(bigger_ngrams['ngrams']))))\n",
    "        else:\n",
    "            d_for_ngrams['c-value'].append(np.log2(len(elem.split()) * d[df['ngram'].loc[i]]['ngrams'][elem]))\n",
    "    \n",
    "    return d_for_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, которая для каждого ngram записывает в словарь контекстные слова: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_w_context(c_value_250, pos, PATH_bigrams):\n",
    "    w_context = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    for file in tqdm_notebook(os.listdir(PATH_bigrams)):\n",
    "        with open(PATH_bigrams + file, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "            for elem in c_value_250['ngram']:\n",
    "                pat = re.compile('(\\w+ )?{} (\\w+)?'.format(elem))\n",
    "                res = re.findall(pat, text)\n",
    "                for words in res:\n",
    "                    for w in words:\n",
    "                        w_context[elem][w] += 1\n",
    "    return w_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для подсчета количества ngrams, с которыми встречаются извлеченные контекстные слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_w_count(w_context, pos):\n",
    "    w_count = defaultdict(lambda: 0)\n",
    "    \n",
    "    for ngram in w_context:\n",
    "        words = []\n",
    "        for w in w_context[ngram]:\n",
    "            if w not in words:\n",
    "                w_count[w] += 1\n",
    "                words.append(w)\n",
    "                \n",
    "    return w_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nc_value(i, w_context, w_count, c_value_250):\n",
    "    context_words = 0\n",
    "    \n",
    "    for elem in set(w_context[c_value_250['ngram'].loc[i]]):\n",
    "        count_elem = 0\n",
    "        \n",
    "        for other in w_context[c_value_250['ngram'].loc[i]]:\n",
    "            if elem == other:\n",
    "                count_elem += 1\n",
    "        \n",
    "        context_words += count_elem * w_count[elem] / len(w_count)\n",
    "    \n",
    "    return 0.8 * c_value_250['c-value'].loc[i] + 0.2 * context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_c_nc_value(pos, d, PATH_bigrams, PATH2):\n",
    "    \n",
    "    df = pd.read_csv('{}/bigrams_{}.txt'.format(PATH_bigrams, pos), sep='\\t',\n",
    "                   names=['id', 'freq', 'range', 'ngram'], index_col=False)\n",
    "    \n",
    "    df['c-value'] = 0\n",
    "    d_for_ngrams = {'ngram': [], 'c-value': []}\n",
    "    \n",
    "    df, d_for_ngrams = count_c_value_for_ngrams(d, df, d_for_ngrams)\n",
    "    \n",
    "    c_value_all = pd.concat([df[['ngram','c-value']],\n",
    "                             pd.DataFrame(d_for_ngrams)]).sort_values('c-value', ascending=False)\n",
    "    c_value_all.to_csv('/home/zu_ann/jahresarbeit/testfile/c_value_{}.txt'.format(pos),\n",
    "                           sep='\\t')\n",
    "    \n",
    "    c_value_250 = c_value_all[0:249]\n",
    "\n",
    "    w_context = create_w_context(c_value_250, pos, PATH_bigrams)\n",
    "    \n",
    "    w_count = create_w_count(w_context, pos)\n",
    "    \n",
    "    c_value_250['nc-value'] = 0\n",
    "    \n",
    "    for i in tqdm_notebook(c_value_250.index):\n",
    "        c_value_250['nc-value'].loc[i] = count_nc_value(i, w_context, w_count, c_value_250)\n",
    "    \n",
    "    c_value_250.sort_values('nc-value', ascending=False).to_csv('{}/nc_value_{}.txt'.format(PATH2, pos),\n",
    "                           sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = ['s_v', 'v_s', 'pr_s', 's_pr', 'a_s', 'adv_v', 'v_pr']\n",
    "for pos in tqdm_notebook(pos_list):\n",
    "    count_c_nc_value(pos, d, PATH_bigrams, PATH2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате для каждого морфологического шаблона (Verb + Noun, Noun + Verb, Prep + Noun, Noun + Prep, Verb + Prep, Adj + Noun, Adv + Verb) были получены списки bigrams, отсортированные с использованием перечисленных методов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
